{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/chiquynhdang03/Dang-Quynh-Chi/blob/main/BigQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-01-15T13:38:29.578126Z",
     "iopub.status.busy": "2026-01-15T13:38:29.577936Z",
     "iopub.status.idle": "2026-01-15T13:38:44.189021Z",
     "shell.execute_reply": "2026-01-15T13:38:44.188123Z"
    },
    "id": "9Uaucm-sIwHE",
    "outputId": "be33f7dc-15d3-4aa5-e587-759417fd4a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hostedtoolcache/Python/3.10.19/x64/lib/python3.10/site-packages/google/api_core/_python_version_support.py:275: FutureWarning: You are using a Python version (3.10.19) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/tmp/ipykernel_2168/1912353058.py:122: FutureWarning: read_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.read_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.read_gbq\n",
      "  df_latest = pd.read_gbq(sql, project_id=GOOGLE_CLOUD_PROJECT_ID, credentials=creds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting BigQuery Update Process ---\n",
      "Authentication successful.\n",
      "Querying BigQuery for the last processed date...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from last date in BigQuery: 2026-01-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading 96 cleaned rows to BigQuery...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2168/1912353058.py:168: FutureWarning: to_gbq is deprecated and will be removed in a future version. Please use pandas_gbq.to_gbq instead: https://pandas-gbq.readthedocs.io/en/latest/api.html#pandas_gbq.to_gbq\n",
      "  processed_df.to_gbq(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded data to BigQuery.\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Environment Setup ---\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB_ENV = True\n",
    "    print(\"Running in Google Colab environment.\")\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    COLAB_ENV = False\n",
    "    print(\"Not running in Google Colab environment.\")\n",
    "\n",
    "# --- Core Imports ---\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import io\n",
    "import ssl\n",
    "\n",
    "# --- Google API Imports ---\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "# No longer need MediaIoBaseUpload/Download for the main data file\n",
    "\n",
    "# --- Constants for Data Cleaning & Processing ---\n",
    "# (This entire section of lists and dictionaries remains exactly the same)\n",
    "COMPLAINT_TYPES_TO_EXCLUDE = [ 'Adopt-A-Basket', 'Advocate - Other', 'Advocate-Co-opCondo Abatement', 'Advocate-Prop Refunds/Credits', 'Animal Facility - No Permit', 'Appliance', 'Beach/Pool/Sauna Complaint', 'Bench', 'Bike Rack', 'Bike Rack Condition', 'Borough Office', 'Building Condition', 'Building Marshals office', 'Bus Stop Shelter Placement', 'Calorie Labeling', 'Collection Truck Noise', 'Construction Safety Enforcement', 'Cooling Tower', 'COVID-19 Non-essential Construction', 'Dept of Investigations', 'Derelict Bicycle', 'Dirty Condition', 'Disorderly Youth', 'Dispatched Taxi Complaint', 'DOF Parking - Tax Exemption', 'DOF Property - Owner Issue', 'DOF Property - Payment Issue', 'DOF Property - Property Value', 'DOF Property - Reduction Issue', 'DOF Property - Request Copy', 'DOF Property - RPIE Issue', 'DOF Property - Update Account', 'DPR Internal', 'DRIE', 'DSNY Internal', 'Dumpster Complaint', 'Executive Inspections', 'Facades', 'Face Covering Violation', 'Ferry Complaint', 'Ferry Inquiry', 'For Hire Vehicle Report', 'Found Property', 'General', 'Green Taxi Complaint', 'Green Taxi Report', 'Harboring Bees/Wasps', 'Heat/Hot Water', 'Highway Condition', 'Highway Sign - Dangling', 'Home Delivered Meal - Missed Delivery', 'Homeless Encampment', 'Homeless Street Condition', 'Housing - Low Income Senior', 'Housing Options', 'Illegal Animal Kept as Pet', 'Illegal Animal Sold', 'Incorrect Data', 'Institution Disposal Complaint', 'Internal Code', 'Lifeguard', 'Literature Request', 'Mass Gathering Complaint', 'Miscellaneous Categories', 'Municipal Parking Facility', 'Noise - House of Worship', 'NonCompliance with Phased Reopening', 'Oil or Gas Spill', 'Other Enforcement', 'OUTSITE BUILDING', 'Overflowing Litter Baskets', 'Paint/Plaster', 'Plant', 'Posting Advertisement', 'Private or Charter School Reopening', 'Private School Vaccine Mandate Non-Compliance', 'Public Payphone Complaint', 'Public Toilet', 'Quality of Life', 'Radioactive Material', 'Recycling Basket Complaint', 'Recycling Enforcement', 'Retailer Complaint', 'SCRIE', 'Seasonal Collection', 'Senior Center Complaint', 'Sewer Maintenance', 'Single Occupancy Bathroom', 'Snow', 'Snow Removal', 'Special Operations', 'Squeegee', 'Storm', 'Sustainability Enforcement', 'Sweeping/Inadequate', 'Sweeping/Missed', 'Tanning', 'Tattooing', 'Taxi Licensee Complaint', 'Taxpayer Advocate Inquiry', 'Unsanitary Animal Facility', 'Unsanitary Animal Pvt Property', 'Uprooted Stump', 'Vacant Lot', 'Vaccine Mandate Non-Compliance', 'Water Leak', 'Water Maintenance', 'Window Guard', 'Wood Pile Remaining', 'X-Ray Machine/Equipment' ]\n",
    "COMPLAINT_TYPE_MERGE_MAP = { 'Animal-Abuse': 'Animal Abuse', 'Derelict Vehicle': 'Derelict Vehicles', 'Electrical': 'ELECTRIC', 'ELEVATOR': 'Elevator', 'Litter Basket / Request': 'Litter Basket Request', 'PLUMBING': 'Plumbing', 'Smoking': 'Smoking or Vaping' }\n",
    "CONSUMER_COMPLAINT_DESCRIPTORS_TO_DELETE = [ 'Retail Store', 'Sidewalk Cafe', 'Other', 'False Advertising', 'Exchange/Refund/Return', 'Locksmith', 'Car Wash', 'Department Store or Megastore', 'Barber Shop, Beauty Salon, or Nail Salon', 'Damaged Vehicle', 'Non-Delivery Goods/Services', 'Unlicensed', 'Car Not Available', 'Non-Delivery of Papers', 'Furniture Store', 'Receipt Incomplete/Not Given', 'Home Heating Oil Company', 'Auction House or Auctioneer', 'Scale Dealer/Repairer', 'Smoking, Cigar or Vape Store', 'Moving Company', 'Secondhand Dealer', 'Bail Bond Agent', 'Catering Establishment', 'Home Appliance Store', 'Publishing Company', 'House/Property Damaged', 'Contract Dispute', 'Laundry', 'Wholesale Food Market', 'Jewelry Appraiser', 'Disabled Device Dealer', 'Horse Drawn Carriage', 'Going Out of Business', 'Door Open with Air Conditioning On', 'Laundromat', 'Gaming Cafe', 'Funeral Home', 'Gas Station', 'Bingo Hall', 'Dealer in Products for the Disabled', 'Hardware Store', 'Pet Store', 'High Pressure to Take on Loan/Debt', 'Debt Not Owed', 'Landlord or Real Estate Agent', 'Jewelry Store', 'Billing Dispute', 'Documents/Paperwork Missing', 'Illegal/Unfair Booting', 'Over Capacity', 'Price Not Posted', 'Rates Not Posted', 'Lost Property', 'Mandatory Tip', 'Paid in Advance', 'Scale Inaccurate/Broken', 'Used Goods Dealer', 'Shipping Company', 'Vocational or Trade School', 'Harassment', 'Damaged/Defective Goods', 'Overcharge' ]\n",
    "CONSUMER_COMPLAINT_DESCRIPTOR_MAP = { 'Bodega/Deli/Supermarket': 'Bodega, Deli, or Convenience Store', 'Garage/Parking Lot': 'Garage or Parking Lot', 'Ticket Broker': 'Ticket Seller', 'Car Dealer - Used': 'Used Car Dealer', 'Hotel': 'Hotel or Motel', 'Immigration Services': 'Immigration Services Provider', 'Mail Order': 'Online or Mail Order', 'Stoop Line': 'Stoop Line Stand', 'Tour Company': 'Tour Guide', 'Tax Preparer': 'Tax Preparation Services', 'For-profit College': 'For-Profit College or University' }\n",
    "VENDOR_DESCRIPTORS_TO_RECATEGORIZE = ['Vendor', 'General Vendor', 'Street Fair Vendor']\n",
    "COLUMNS_TO_DELETE = [ 'address_type', 'city', 'facility_type', 'due_date', 'resolution_action_updated_date', 'bbl', 'borough', 'x_coordinate_state_plane', 'y_coordinate_state_plane', 'park_facility_name', 'park_borough', 'vehicle_type', 'taxi_company_borough', 'taxi_pick_up_location', 'bridge_highway_name', 'bridge_highway_direction', 'road_ramp', 'bridge_highway_segment', 'location' ]\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "NYC_OPEN_DATA_RESOURCE_URL = \"https://data.cityofnewyork.us/resource/erm2-nwe9.csv\"\n",
    "API_LIMIT_PER_REQUEST = 1000\n",
    "ZIP_CODES_TO_INCLUDE = [10004, 10005, 10006, 10007, 10038, 10280, 10282, 10013, 10002]\n",
    "DEFAULT_INITIAL_FETCH_DATE = \"2018-07-01\"\n",
    "\n",
    "# <<< NEW BIGQUERY CONFIGURATION >>>\n",
    "GOOGLE_CLOUD_PROJECT_ID = \"stable-liberty-426016-d2\"\n",
    "BIGQUERY_TABLE_ID = \"nyc_311_data.complaints\"\n",
    "\n",
    "# Dynamic path for the service account key\n",
    "if COLAB_ENV:\n",
    "    SERVICE_ACCOUNT_KEY_FILE = \"/content/drive/Shareddrives/311_Complaint_Data/service_account_key.json\"\n",
    "else:\n",
    "    SERVICE_ACCOUNT_KEY_FILE = \"service_account_key.json\"\n",
    "\n",
    "# --- Helper Functions (No changes) ---\n",
    "def process_and_clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # This function is unchanged\n",
    "    if df.empty: return df\n",
    "    df = df[~((df['complaint_type'] == 'Consumer Complaint') & (df['descriptor'].isin(CONSUMER_COMPLAINT_DESCRIPTORS_TO_DELETE)))]\n",
    "    df['complaint_type'] = df['complaint_type'].replace(COMPLAINT_TYPE_MERGE_MAP)\n",
    "    consumer_mask = df['complaint_type'] == 'Consumer Complaint'\n",
    "    df.loc[consumer_mask, 'descriptor'] = df.loc[consumer_mask, 'descriptor'].replace(CONSUMER_COMPLAINT_DESCRIPTOR_MAP)\n",
    "    vendor_mask = (df['complaint_type'] == 'Consumer Complaint') & (df['descriptor'].isin(VENDOR_DESCRIPTORS_TO_RECATEGORIZE))\n",
    "    df.loc[vendor_mask, 'complaint_type'] = 'Vendor Enforcement'\n",
    "    df.drop(columns=COLUMNS_TO_DELETE, inplace=True, errors='ignore')\n",
    "    date_cols = ['created_date', 'closed_date', 'resolution_action_updated_date']\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "            df[col] = df[col].dt.tz_convert('America/New_York').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return df\n",
    "\n",
    "def fetch_nyc_data_incremental(start_date_str: str, end_date_str: str = None) -> pd.DataFrame:\n",
    "    # This function is unchanged\n",
    "    all_fetched_dfs = []\n",
    "    offset = 0\n",
    "    more_data_available = True\n",
    "    if end_date_str: date_filter = f\"created_date >= '{start_date_str}T00:00:00.000' AND created_date <= '{end_date_str}T23:59:59.999'\"\n",
    "    else: date_filter = f\"created_date >= '{start_date_str}T00:00:00.000'\"\n",
    "    community_board_filter = \"contains(community_board, '01 MANHATTAN')\"\n",
    "    where_clause = f\"{date_filter} AND {community_board_filter}\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=10, backoff_factor=2, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    while more_data_available:\n",
    "        params = {'$limit': API_LIMIT_PER_REQUEST, '$offset': offset, '$where': where_clause, '$order': 'created_date ASC'}\n",
    "        try:\n",
    "            response = session.get(NYC_OPEN_DATA_RESOURCE_URL, params=params, timeout=90)\n",
    "            response.raise_for_status()\n",
    "            if not response.text.strip(): more_data_available = False; continue\n",
    "            page_df = pd.read_csv(io.StringIO(response.text))\n",
    "            if not page_df.empty:\n",
    "                all_fetched_dfs.append(page_df)\n",
    "                offset += len(page_df)\n",
    "                if len(page_df) < API_LIMIT_PER_REQUEST: more_data_available = False\n",
    "            else: more_data_available = False\n",
    "            time.sleep(1)\n",
    "        except (requests.exceptions.RequestException, pd.errors.EmptyDataError) as e:\n",
    "            break\n",
    "    if not all_fetched_dfs: return pd.DataFrame()\n",
    "    return pd.concat(all_fetched_dfs, ignore_index=True)\n",
    "\n",
    "# <<< MODIFIED MAIN FUNCTION FOR BIGQUERY >>>\n",
    "def update_bigquery_data():\n",
    "    \"\"\"Main function to fetch, clean, and upload data to BigQuery.\"\"\"\n",
    "    print(\"--- Starting BigQuery Update Process ---\")\n",
    "\n",
    "    # Authenticate using the service account\n",
    "    try:\n",
    "        creds = service_account.Credentials.from_service_account_file(\n",
    "            SERVICE_ACCOUNT_KEY_FILE,\n",
    "            scopes=[\"https://www.googleapis.com/auth/cloud-platform\", \"https://www.googleapis.com/auth/drive\"]\n",
    "        )\n",
    "        print(\"Authentication successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Authentication failed: {e}\"); return\n",
    "\n",
    "    # Determine the start date for fetching\n",
    "    # This logic now needs to query BigQuery to get the latest date\n",
    "    last_processed_date_str = DEFAULT_INITIAL_FETCH_DATE\n",
    "    try:\n",
    "        print(\"Querying BigQuery for the last processed date...\")\n",
    "        sql = f\"SELECT MAX(created_date) as max_date FROM `{GOOGLE_CLOUD_PROJECT_ID}.{BIGQUERY_TABLE_ID}`\"\n",
    "        df_latest = pd.read_gbq(sql, project_id=GOOGLE_CLOUD_PROJECT_ID, credentials=creds)\n",
    "        if not df_latest.empty and pd.notna(df_latest['max_date'][0]):\n",
    "            last_processed_date = pd.to_datetime(df_latest['max_date'][0])\n",
    "            last_processed_date_str = last_processed_date.strftime('%Y-%m-%d')\n",
    "            print(f\"Resuming from last date in BigQuery: {last_processed_date_str}\")\n",
    "        else:\n",
    "            print(\"BigQuery table is empty or has no dates. Using default start date.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get last date from BigQuery: {e}. Using default start date.\")\n",
    "\n",
    "    start_date = datetime.strptime(last_processed_date_str, '%Y-%m-%d') + timedelta(days=1)\n",
    "    if start_date < datetime.strptime(DEFAULT_INITIAL_FETCH_DATE, '%Y-%m-%d'):\n",
    "        start_date = datetime.strptime(DEFAULT_INITIAL_FETCH_DATE, '%Y-%m-%d')\n",
    "    end_date = datetime.now()\n",
    "\n",
    "    # Fetch data in yearly chunks (this logic remains the same)\n",
    "    all_chunks_dfs = []\n",
    "    for year in range(start_date.year, end_date.year + 1):\n",
    "        chunk_start_date = max(start_date, datetime(year, 1, 1))\n",
    "        chunk_end_date = min(end_date, datetime(year, 12, 31))\n",
    "        if chunk_start_date > chunk_end_date: continue\n",
    "        chunk_df = fetch_nyc_data_incremental(\n",
    "            chunk_start_date.strftime('%Y-%m-%d'), chunk_end_date.strftime('%Y-%m-%d'))\n",
    "        if not chunk_df.empty: all_chunks_dfs.append(chunk_df)\n",
    "\n",
    "    if not all_chunks_dfs:\n",
    "        print(\"\\nNo new data found to upload. Process finished.\"); return\n",
    "\n",
    "    new_data_df = pd.concat(all_chunks_dfs, ignore_index=True)\n",
    "\n",
    "    # Filter and clean the new data (this logic remains the same)\n",
    "    new_data_df['incident_zip'] = pd.to_numeric(new_data_df['incident_zip'], errors='coerce')\n",
    "    filtered_df = new_data_df[new_data_df['incident_zip'].isin(ZIP_CODES_TO_INCLUDE)].copy()\n",
    "    filtered_df = filtered_df[~filtered_df['complaint_type'].isin(COMPLAINT_TYPES_TO_EXCLUDE)]\n",
    "    if filtered_df.empty:\n",
    "        print(\"No data remains after filtering. Process finished.\"); return\n",
    "\n",
    "    processed_df = process_and_clean_data(filtered_df)\n",
    "\n",
    "    # De-duplicate before uploading\n",
    "    if 'unique_key' in processed_df.columns:\n",
    "        processed_df.drop_duplicates(subset=['unique_key'], inplace=True, keep='last')\n",
    "\n",
    "    # Upload the final, processed DataFrame to BigQuery\n",
    "    print(f\"\\nUploading {len(processed_df)} cleaned rows to BigQuery...\")\n",
    "    try:\n",
    "        processed_df.to_gbq(\n",
    "            destination_table=BIGQUERY_TABLE_ID,\n",
    "            project_id=GOOGLE_CLOUD_PROJECT_ID,\n",
    "            if_exists='append', # This adds the new rows without deleting old ones\n",
    "            credentials=creds\n",
    "        )\n",
    "        print(\"Successfully uploaded data to BigQuery.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Failed to upload data to BigQuery: {e}\")\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    update_bigquery_data()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMv5OKAiTvg/YPcph2BMJTU",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
