{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/chiquynhdang03/Dang-Quynh-Chi/blob/main/gg_sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-08T21:53:26.537568Z",
     "iopub.status.busy": "2025-08-08T21:53:26.537376Z",
     "iopub.status.idle": "2025-08-08T21:53:27.213487Z",
     "shell.execute_reply": "2025-08-08T21:53:27.212747Z"
    },
    "id": "bgDnNxjzRNL8",
    "outputId": "5a6626bb-01b8-42d9-a539-7531296e9e52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Google Colab environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Google Sheet Update Process with Yearly Chunking ---\n",
      "Authenticating with Google Drive...\n",
      "FATAL: Google Drive authentication failed: Expecting value: line 2 column 1 (char 1)\n",
      "Attempted to use key file path: service_account_key.json\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Environment Setup ---\n",
    "# This block detects the environment and mounts Drive ONLY if in Colab.\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB_ENV = True\n",
    "    print(\"Running in Google Colab environment.\")\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    COLAB_ENV = False\n",
    "    print(\"Not running in Google Colab environment.\")\n",
    "\n",
    "# --- Install Dependencies ---\n",
    "if COLAB_ENV:\n",
    "    print(\"Installing/Upgrading Python dependencies in Colab...\")\n",
    "    # !pip install pandas requests google-auth google-api-python-client urllib3 openpyxl --upgrade\n",
    "    print(\"Dependencies check complete.\")\n",
    "\n",
    "# --- Core Imports ---\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import io\n",
    "import ssl\n",
    "\n",
    "# --- Google API Imports ---\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# --- Constants for Data Cleaning & Processing ---\n",
    "# 1. List of all Complaint Types to EXCLUDE from the data.\n",
    "COMPLAINT_TYPES_TO_EXCLUDE = [\n",
    "    'Adopt-A-Basket', 'Advocate - Other', 'Advocate-Co-opCondo Abatement',\n",
    "    'Advocate-Prop Refunds/Credits', 'Animal Facility - No Permit', 'Appliance',\n",
    "    'Beach/Pool/Sauna Complaint', 'Bench', 'Bike Rack', 'Bike Rack Condition',\n",
    "    'Borough Office', 'Building Condition', 'Building Marshals office',\n",
    "    'Bus Stop Shelter Placement', 'Calorie Labeling', 'Collection Truck Noise',\n",
    "    'Construction Safety Enforcement', 'Cooling Tower', 'COVID-19 Non-essential Construction',\n",
    "    'Dept of Investigations', 'Derelict Bicycle', 'Dirty Condition', 'Disorderly Youth',\n",
    "    'Dispatched Taxi Complaint', 'DOF Parking - Tax Exemption', 'DOF Property - Owner Issue',\n",
    "    'DOF Property - Payment Issue', 'DOF Property - Property Value', 'DOF Property - Reduction Issue',\n",
    "    'DOF Property - Request Copy', 'DOF Property - RPIE Issue', 'DOF Property - Update Account',\n",
    "    'DPR Internal', 'DRIE', 'DSNY Internal', 'Dumpster Complaint', 'Executive Inspections',\n",
    "    'Facades', 'Face Covering Violation', 'Ferry Complaint', 'Ferry Inquiry',\n",
    "    'For Hire Vehicle Report', 'Found Property', 'General', 'Green Taxi Complaint',\n",
    "    'Green Taxi Report', 'Harboring Bees/Wasps', 'Heat/Hot Water', 'Highway Condition',\n",
    "    'Highway Sign - Dangling', 'Home Delivered Meal - Missed Delivery', 'Homeless Encampment',\n",
    "    'Homeless Street Condition', 'Housing - Low Income Senior', 'Housing Options',\n",
    "    'Illegal Animal Kept as Pet', 'Illegal Animal Sold', 'Incorrect Data',\n",
    "    'Institution Disposal Complaint', 'Internal Code', 'Lifeguard', 'Literature Request',\n",
    "    'Mass Gathering Complaint', 'Miscellaneous Categories', 'Municipal Parking Facility',\n",
    "    'Noise - House of Worship', 'NonCompliance with Phased Reopening', 'Oil or Gas Spill',\n",
    "    'Other Enforcement', 'OUTSITE BUILDING', 'Overflowing Litter Baskets', 'Paint/Plaster',\n",
    "    'Plant', 'Posting Advertisement', 'Private or Charter School Reopening',\n",
    "    'Private School Vaccine Mandate Non-Compliance', 'Public Payphone Complaint',\n",
    "    'Public Toilet', 'Quality of Life', 'Radioactive Material', 'Recycling Basket Complaint',\n",
    "    'Recycling Enforcement', 'Retailer Complaint', 'SCRIE', 'Seasonal Collection',\n",
    "    'Senior Center Complaint', 'Sewer Maintenance', 'Single Occupancy Bathroom', 'Snow',\n",
    "    'Snow Removal', 'Special Operations', 'Squeegee', 'Storm', 'Sustainability Enforcement',\n",
    "    'Sweeping/Inadequate', 'Sweeping/Missed', 'Tanning', 'Tattooing', 'Taxi Licensee Complaint',\n",
    "    'Taxpayer Advocate Inquiry', 'Unsanitary Animal Facility', 'Unsanitary Animal Pvt Property',\n",
    "    'Uprooted Stump', 'Vacant Lot', 'Vaccine Mandate Non-Compliance', 'Water Leak',\n",
    "    'Water Maintenance', 'Window Guard', 'Wood Pile Remaining', 'X-Ray Machine/Equipment'\n",
    "]\n",
    "# 2. Dictionary to MERGE different Complaint Type names.\n",
    "COMPLAINT_TYPE_MERGE_MAP = {\n",
    "    'Animal-Abuse': 'Animal Abuse',\n",
    "    'Derelict Vehicle': 'Derelict Vehicles',\n",
    "    'Electrical': 'ELECTRIC',\n",
    "    'ELEVATOR': 'Elevator',\n",
    "    'Litter Basket / Request': 'Litter Basket Request',\n",
    "    'PLUMBING': 'Plumbing',\n",
    "    'Smoking': 'Smoking or Vaping'\n",
    "}\n",
    "# 3. List of Descriptors to DELETE within the 'Consumer Complaint' type.\n",
    "CONSUMER_COMPLAINT_DESCRIPTORS_TO_DELETE = [\n",
    "    'Retail Store', 'Sidewalk Cafe', 'Other', 'False Advertising', 'Exchange/Refund/Return',\n",
    "    'Locksmith', 'Car Wash', 'Department Store or Megastore', 'Barber Shop, Beauty Salon, or Nail Salon',\n",
    "    'Damaged Vehicle', 'Non-Delivery Goods/Services', 'Unlicensed', 'Car Not Available',\n",
    "    'Non-Delivery of Papers', 'Furniture Store', 'Receipt Incomplete/Not Given',\n",
    "    'Home Heating Oil Company', 'Auction House or Auctioneer', 'Scale Dealer/Repairer',\n",
    "    'Smoking, Cigar or Vape Store', 'Moving Company', 'Secondhand Dealer', 'Bail Bond Agent',\n",
    "    'Catering Establishment', 'Home Appliance Store', 'Publishing Company', 'House/Property Damaged',\n",
    "    'Contract Dispute', 'Laundry', 'Wholesale Food Market', 'Jewelry Appraiser',\n",
    "    'Disabled Device Dealer', 'Horse Drawn Carriage', 'Going Out of Business',\n",
    "    'Door Open with Air Conditioning On', 'Laundromat', 'Gaming Cafe', 'Funeral Home',\n",
    "    'Gas Station', 'Bingo Hall', 'Dealer in Products for the Disabled', 'Hardware Store',\n",
    "    'Pet Store', 'High Pressure to Take on Loan/Debt', 'Debt Not Owed', 'Landlord or Real Estate Agent',\n",
    "    'Jewelry Store', 'Billing Dispute', 'Documents/Paperwork Missing', 'Illegal/Unfair Booting',\n",
    "    'Over Capacity', 'Price Not Posted', 'Rates Not Posted', 'Lost Property', 'Mandatory Tip',\n",
    "    'Paid in Advance', 'Scale Inaccurate/Broken', 'Used Goods Dealer', 'Shipping Company',\n",
    "    'Vocational or Trade School', 'Harassment', 'Damaged/Defective Goods', 'Overcharge'\n",
    "]\n",
    "# 4. Dictionary to MERGE Descriptors within the 'Consumer Complaint' type.\n",
    "CONSUMER_COMPLAINT_DESCRIPTOR_MAP = {\n",
    "    'Bodega/Deli/Supermarket': 'Bodega, Deli, or Convenience Store',\n",
    "    'Garage/Parking Lot': 'Garage or Parking Lot',\n",
    "    'Ticket Broker': 'Ticket Seller',\n",
    "    'Car Dealer - Used': 'Used Car Dealer',\n",
    "    'Hotel': 'Hotel or Motel',\n",
    "    'Immigration Services': 'Immigration Services Provider',\n",
    "    'Mail Order': 'Online or Mail Order',\n",
    "    'Stoop Line': 'Stoop Line Stand',\n",
    "    'Tour Company': 'Tour Guide',\n",
    "    'Tax Preparer': 'Tax Preparation Services',\n",
    "    'For-profit College': 'For-Profit College or University'\n",
    "}\n",
    "# 5. List of Descriptors to RE-CATEGORIZE from 'Consumer Complaint' to 'Vendor Enforcement'.\n",
    "VENDOR_DESCRIPTORS_TO_RECATEGORIZE = ['Vendor', 'General Vendor', 'Street Fair Vendor']\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "NYC_OPEN_DATA_RESOURCE_URL = \"https://data.cityofnewyork.us/resource/erm2-nwe9.csv\"\n",
    "API_LIMIT_PER_REQUEST = 1000\n",
    "ZIP_CODES_TO_INCLUDE = [10004, 10005, 10006, 10007, 10038, 10280, 10282, 10013, 10002]\n",
    "GOOGLE_DRIVE_FOLDER_ID = \"0AI4Egw2Y1IwhUk9PVA\"\n",
    "GSHEET_FILE_NAME = \"CB1_311_Complaint_Data_GSheet\"\n",
    "GSHEET_MIMETYPE = \"application/vnd.google-apps.spreadsheet\"\n",
    "GSHEET_DRIVE_FILE_NAME = \"CB1_311_Complaint_Data_GSheet\"\n",
    "DEFAULT_INITIAL_FETCH_DATE = \"2018-07-01\"\n",
    "PROGRESS_FILE_NAME = \"last_processed_date.txt\"\n",
    "PROGRESS_FILE_MIMETYPE = \"text/plain\"\n",
    "\n",
    "# <<< THIS IS THE FIRST KEY CHANGE >>>\n",
    "# The path to the service account key is now set dynamically.\n",
    "if COLAB_ENV:\n",
    "    # This path is used when running in Google Colab.\n",
    "    SERVICE_ACCOUNT_KEY_FILE = \"/content/drive/Shareddrives/311_Complaint_Data/service_account_key.json\"\n",
    "else:\n",
    "    # This path is used when running in GitHub Actions.\n",
    "    SERVICE_ACCOUNT_KEY_FILE = \"service_account_key.json\"\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def read_last_processed_date(drive_service) -> str:\n",
    "    \"\"\"Reads the last processed date from the progress file in Google Drive.\"\"\"\n",
    "    print(\"Attempting to read last processed date from Google Drive...\")\n",
    "    try:\n",
    "        file_list_response = drive_service.files().list(\n",
    "            q=f\"'{GOOGLE_DRIVE_FOLDER_ID}' in parents and trashed=false and name='{PROGRESS_FILE_NAME}'\",\n",
    "            spaces='drive', fields='files(id, name)'\n",
    "        ).execute()\n",
    "        file_items = file_list_response.get('files', [])\n",
    "\n",
    "        if file_items:\n",
    "            progress_file_id = file_items[0]['id']\n",
    "            print(f\"Found progress file (ID: {progress_file_id}). Downloading...\")\n",
    "            request = drive_service.files().get_media(fileId=progress_file_id)\n",
    "            file_content_bytes = io.BytesIO()\n",
    "            downloader = MediaIoBaseDownload(file_content_bytes, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "            file_content_bytes.seek(0)\n",
    "            date_str = file_content_bytes.getvalue().decode('utf-8').strip()\n",
    "            datetime.strptime(date_str, '%Y-%m-%d')\n",
    "            print(f\"Resuming from last processed date: {date_str}\")\n",
    "            return date_str\n",
    "        print(f\"No progress file found. Starting from default: {DEFAULT_INITIAL_FETCH_DATE}\")\n",
    "        return DEFAULT_INITIAL_FETCH_DATE\n",
    "    except (ValueError, HttpError, Exception) as e:\n",
    "        print(f\"Error reading progress file: {e}. Starting from default.\")\n",
    "        return DEFAULT_INITIAL_FETCH_DATE\n",
    "\n",
    "def write_last_processed_date(date_str: str, drive_service):\n",
    "    \"\"\"Writes the last successfully processed date to the progress file.\"\"\"\n",
    "    try:\n",
    "        progress_buffer = io.BytesIO(date_str.encode('utf-8'))\n",
    "        file_list_response = drive_service.files().list(\n",
    "            q=f\"'{GOOGLE_DRIVE_FOLDER_ID}' in parents and trashed=false and name='{PROGRESS_FILE_NAME}'\",\n",
    "            spaces='drive', fields='files(id, name)'\n",
    "        ).execute()\n",
    "        file_items = file_list_response.get('files', [])\n",
    "\n",
    "        media = MediaIoBaseUpload(progress_buffer, mimetype=PROGRESS_FILE_MIMETYPE, resumable=True)\n",
    "        if file_items:\n",
    "            drive_service.files().update(fileId=file_items[0]['id'], media_body=media).execute()\n",
    "        else:\n",
    "            file_metadata = {'name': PROGRESS_FILE_NAME, 'parents': [GOOGLE_DRIVE_FOLDER_ID]}\n",
    "            drive_service.files().create(body=file_metadata, media_body=media, supportsAllDrives=True).execute()\n",
    "        print(f\"Progress saved: Last processed date is now {date_str}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save progress to Drive file: {e}\")\n",
    "\n",
    "def process_and_clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Performs all detailed, multi-step data cleaning and processing locally.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    print(\"\\n--- Starting Comprehensive Local Data Processing ---\")\n",
    "\n",
    "    initial_rows = len(df)\n",
    "    condition_to_delete = (df['complaint_type'] == 'Consumer Complaint') & \\\n",
    "                          (df['descriptor'].isin(CONSUMER_COMPLAINT_DESCRIPTORS_TO_DELETE))\n",
    "    df = df[~condition_to_delete]\n",
    "    print(f\"Deleted {initial_rows - len(df)} rows based on 'Consumer Complaint' descriptor list.\")\n",
    "\n",
    "    print(\"Merging general complaint types...\")\n",
    "    df['complaint_type'] = df['complaint_type'].replace(COMPLAINT_TYPE_MERGE_MAP)\n",
    "\n",
    "    print(\"Merging specific 'Consumer Complaint' descriptors...\")\n",
    "    consumer_mask = df['complaint_type'] == 'Consumer Complaint'\n",
    "    df.loc[consumer_mask, 'descriptor'] = df.loc[consumer_mask, 'descriptor'].replace(CONSUMER_COMPLAINT_DESCRIPTOR_MAP)\n",
    "\n",
    "    print(\"Re-categorizing vendor complaints from 'Consumer Complaint' to 'Vendor Enforcement'...\")\n",
    "    vendor_mask = (df['complaint_type'] == 'Consumer Complaint') & \\\n",
    "                  (df['descriptor'].isin(VENDOR_DESCRIPTORS_TO_RECATEGORIZE))\n",
    "    df.loc[vendor_mask, 'complaint_type'] = 'Vendor Enforcement'\n",
    "\n",
    "    print(\"Standardizing date formats...\")\n",
    "    date_cols = ['created_date', 'closed_date', 'due_date', 'resolution_action_updated_date']\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "            df[col] = df[col].dt.tz_convert('America/New_York').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    print(\"--- Comprehensive Local Data Processing Complete ---\\n\")\n",
    "    return df\n",
    "\n",
    "def fetch_nyc_data_incremental(start_date_str: str, end_date_str: str = None) -> pd.DataFrame:\n",
    "    \"\"\"Fetches a broader dataset from the API filtered only by date and community board.\"\"\"\n",
    "    all_fetched_dfs = []\n",
    "    offset = 0\n",
    "    more_data_available = True\n",
    "\n",
    "    if end_date_str:\n",
    "        date_filter = f\"created_date >= '{start_date_str}T00:00:00.000' AND created_date <= '{end_date_str}T23:59:59.999'\"\n",
    "    else:\n",
    "        date_filter = f\"created_date >= '{start_date_str}T00:00:00.000'\"\n",
    "\n",
    "    community_board_filter = \"contains(community_board, '01 MANHATTAN')\"\n",
    "    where_clause = f\"{date_filter} AND {community_board_filter}\"\n",
    "    print(f\"Using simplified, robust WHERE clause:\\n{where_clause}\")\n",
    "\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=10, backoff_factor=2, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    while more_data_available:\n",
    "        params = {'$limit': API_LIMIT_PER_REQUEST, '$offset': offset, '$where': where_clause, '$order': 'created_date ASC'}\n",
    "        try:\n",
    "            print(f\"Fetching page with offset: {offset}...\")\n",
    "            response = session.get(NYC_OPEN_DATA_RESOURCE_URL, params=params, timeout=90)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            if not response.text.strip():\n",
    "                more_data_available = False\n",
    "                continue\n",
    "\n",
    "            page_df = pd.read_csv(io.StringIO(response.text))\n",
    "            if not page_df.empty:\n",
    "                print(f\"Successfully fetched {len(page_df)} records for the community board.\")\n",
    "                all_fetched_dfs.append(page_df)\n",
    "                offset += len(page_df)\n",
    "                if len(page_df) < API_LIMIT_PER_REQUEST:\n",
    "                    more_data_available = False\n",
    "            else:\n",
    "                more_data_available = False\n",
    "            time.sleep(1)\n",
    "        except (requests.exceptions.RequestException, pd.errors.EmptyDataError) as e:\n",
    "            print(f\"Stopping fetch loop: {e}\")\n",
    "            break\n",
    "\n",
    "    if not all_fetched_dfs:\n",
    "        print(\"No new records for this community board were found from the API.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.concat(all_fetched_dfs, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# --- Main Orchestrator Function ---\n",
    "def update_google_sheet_data():\n",
    "    \"\"\"Main function to authenticate, fetch, process, and upload data.\"\"\"\n",
    "    print(\"--- Starting Google Sheet Update Process with Yearly Chunking ---\")\n",
    "\n",
    "    # <<< THIS IS THE SECOND KEY CHANGE >>>\n",
    "    # The Colab-specific drive.mount() call is no longer needed in this function.\n",
    "    # The script now correctly finds the key file using the dynamic path.\n",
    "    print(\"Authenticating with Google Drive...\")\n",
    "    try:\n",
    "        creds = service_account.Credentials.from_service_account_file(\n",
    "            SERVICE_ACCOUNT_KEY_FILE, scopes=['https://www.googleapis.com/auth/drive'])\n",
    "        drive_service = build('drive', 'v3', credentials=creds)\n",
    "        print(\"Google Drive authentication successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Google Drive authentication failed: {e}\")\n",
    "        print(f\"Attempted to use key file path: {SERVICE_ACCOUNT_KEY_FILE}\")\n",
    "        return\n",
    "\n",
    "    existing_df, file_id_in_drive = pd.DataFrame(), None\n",
    "    try:\n",
    "        file_list_response = drive_service.files().list(\n",
    "            q=f\"'{GOOGLE_DRIVE_FOLDER_ID}' in parents and trashed=false and name='{GSHEET_DRIVE_FILE_NAME}'\",\n",
    "            spaces='drive', fields='files(id, name, mimeType)').execute()\n",
    "        file_items = file_list_response.get('files', [])\n",
    "        if file_items:\n",
    "            file_id_in_drive = file_items[0]['id']\n",
    "            print(f\"Found existing Google Sheet (ID: {file_id_in_drive}). Downloading...\")\n",
    "            request = drive_service.files().export_media(fileId=file_id_in_drive, mimeType='text/csv')\n",
    "            file_content_bytes = io.BytesIO()\n",
    "            downloader = MediaIoBaseDownload(file_content_bytes, request)\n",
    "            done = False\n",
    "            while not done: status, done = downloader.next_chunk()\n",
    "            file_content_bytes.seek(0)\n",
    "            existing_df = pd.read_csv(file_content_bytes)\n",
    "    except (HttpError, pd.errors.EmptyDataError, Exception) as e:\n",
    "        print(f\"Could not load existing sheet: {e}. Starting fresh.\")\n",
    "\n",
    "    last_processed_date_str = read_last_processed_date(drive_service)\n",
    "    start_date = datetime.strptime(last_processed_date_str, '%Y-%m-%d') + timedelta(days=1)\n",
    "    if start_date < datetime.strptime(DEFAULT_INITIAL_FETCH_DATE, '%Y-%m-%d'):\n",
    "        start_date = datetime.strptime(DEFAULT_INITIAL_FETCH_DATE, '%Y-%m-%d')\n",
    "    end_date = datetime.now()\n",
    "\n",
    "    all_chunks_dfs = []\n",
    "    print(\"\\n--- Starting Fetch Process in Yearly Chunks ---\")\n",
    "    for year in range(start_date.year, end_date.year + 1):\n",
    "        chunk_start_date = max(start_date, datetime(year, 1, 1))\n",
    "        chunk_end_date = min(end_date, datetime(year, 12, 31))\n",
    "        if chunk_start_date > chunk_end_date: continue\n",
    "        print(f\"\\n>>> Processing chunk for {year} ({chunk_start_date.strftime('%Y-%m-%d')} to {chunk_end_date.strftime('%Y-%m-%d')})\")\n",
    "        chunk_df = fetch_nyc_data_incremental(\n",
    "            chunk_start_date.strftime('%Y-%m-%d'), chunk_end_date.strftime('%Y-%m-%d'))\n",
    "        if not chunk_df.empty: all_chunks_dfs.append(chunk_df)\n",
    "\n",
    "    if not all_chunks_dfs:\n",
    "        print(\"\\nNo new data found. Process finished.\"); write_last_processed_date(datetime.now().strftime('%Y-%m-%d'), drive_service); return\n",
    "\n",
    "    new_data_df = pd.concat(all_chunks_dfs, ignore_index=True)\n",
    "\n",
    "    # Local Filtering\n",
    "    print(\"\\n--- Starting Local Filtering with Pandas ---\")\n",
    "    new_data_df['incident_zip'] = pd.to_numeric(new_data_df['incident_zip'], errors='coerce')\n",
    "    filtered_df = new_data_df[new_data_df['incident_zip'].isin(ZIP_CODES_TO_INCLUDE)].copy()\n",
    "    print(f\"ZIP Code Filter: Kept {len(filtered_df)} of {len(new_data_df)} rows.\")\n",
    "    initial_rows = len(filtered_df)\n",
    "    filtered_df = filtered_df[~filtered_df['complaint_type'].isin(COMPLAINT_TYPES_TO_EXCLUDE)]\n",
    "    print(f\"Complaint Type Exclusion: Kept {len(filtered_df)} of {initial_rows} rows.\")\n",
    "    print(\"--- Local Filtering Complete ---\\n\")\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(\"No data remains after filtering. Process finished.\"); write_last_processed_date(datetime.now().strftime('%Y-%m-%d'), drive_service); return\n",
    "\n",
    "    processed_df = process_and_clean_data(filtered_df)\n",
    "\n",
    "    if not existing_df.empty:\n",
    "        processed_df = processed_df.reindex(columns=existing_df.columns, fill_value=pd.NA)\n",
    "        existing_df = existing_df.reindex(columns=processed_df.columns, fill_value=pd.NA)\n",
    "        combined_df = pd.concat([existing_df, processed_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = processed_df\n",
    "\n",
    "    if 'unique_key' in combined_df.columns:\n",
    "        combined_df.drop_duplicates(subset=['unique_key'], inplace=True, keep='last')\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    combined_df.to_csv(csv_buffer, index=False, encoding='utf-8')\n",
    "    csv_buffer.seek(0)\n",
    "    media = MediaIoBaseUpload(io.BytesIO(csv_buffer.getvalue().encode('utf-8')), mimetype='text/csv', resumable=True)\n",
    "\n",
    "    try:\n",
    "        if file_id_in_drive:\n",
    "            drive_service.files().update(fileId=file_id_in_drive, media_body=media, supportsAllDrives=True).execute()\n",
    "        else:\n",
    "            file_metadata = {'name': GSHEET_FILE_NAME, 'parents': [GOOGLE_DRIVE_FOLDER_ID], 'mimeType': GSHEET_MIMETYPE}\n",
    "            drive_service.files().create(body=file_metadata, media_body=media, supportsAllDrives=True, fields='id').execute()\n",
    "        write_last_processed_date(datetime.now().strftime('%Y-%m-%d'), drive_service)\n",
    "        print(\"\\n--- Google Sheet Update and Upload Process Completed ---\\n\")\n",
    "    except HttpError as e:\n",
    "        print(f\"FATAL: An error occurred during file upload: {e}\")\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    update_google_sheet_data()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN7oZNYAruGLHEGxgrs/eSl",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
