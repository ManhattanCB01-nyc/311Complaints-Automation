{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/chiquynhdang03/Dang-Quynh-Chi/blob/main/gg_sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-08-04T16:09:12.142979Z",
     "iopub.status.busy": "2025-08-04T16:09:12.142803Z",
     "iopub.status.idle": "2025-08-04T16:09:12.737806Z",
     "shell.execute_reply": "2025-08-04T16:09:12.737173Z"
    },
    "id": "bgDnNxjzRNL8",
    "outputId": "abc4ef85-5523-491f-efa2-0d6cefabcaae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping pip install, dependencies handled by GitHub Actions environment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Google Sheet Update and Google Drive Upload Process (Pure In-Memory) ---\n",
      "Authenticating with Google Drive...\n",
      "Google Drive authentication failed: [Errno 2] No such file or directory: '/content/drive/Shareddrives/311_Complaint_Data/service_account_key.json'\n",
      "Please ensure '/content/drive/Shareddrives/311_Complaint_Data/service_account_key.json' is in the correct Google Drive folder and accessible via mounting.\n",
      "Also check Service Account permissions in Google Cloud Console and Drive folder sharing.\n"
     ]
    }
   ],
   "source": [
    "# --- Initial Environment Setup (MUST BE AT THE TOP) ---\n",
    "COLAB_ENV = False\n",
    "try:\n",
    "    # Attempt to import google.colab to detect Colab environment\n",
    "    from google.colab import drive\n",
    "    COLAB_ENV = True\n",
    "except ImportError:\n",
    "    pass # Not in Colab environment\n",
    "\n",
    "# --- Install Dependencies (For Colab, run this cell first) ---\n",
    "# In Colab, you'd typically run this in a separate cell.\n",
    "# If running in GitHub Actions, these are handled by the workflow's 'Install dependencies' step.\n",
    "if COLAB_ENV:\n",
    "    print(\"Installing/Upgrading Python dependencies in Colab...\")\n",
    "    #!pip install pandas requests google-auth google-api-python-client urllib3 openpyxl --upgrade\n",
    "    print(\"Dependencies installed.\")\n",
    "else:\n",
    "    print(\"Skipping pip install, dependencies handled by GitHub Actions environment.\")\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time # Import time for sleep functionality\n",
    "import io # Import io for string stream stream handling\n",
    "import ssl # NEW IMPORT\n",
    "\n",
    "# NEW IMPORTS for direct Google API client authentication\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload\n",
    "from googleapiclient.errors import HttpError # Import HttpError for specific error handling\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "# Changed to CSV endpoint\n",
    "NYC_OPEN_DATA_RESOURCE_URL = \"https://data.cityofnewyork.us/resource/erm2-nwe9.csv\"\n",
    "API_LIMIT_PER_REQUEST = 1000\n",
    "ZIP_CODES_TO_INCLUDE = [10004, 10005, 10006, 10007, 10038, 10280, 10282, 10013, 10002]\n",
    "\n",
    "# --- Google Sheet File Configuration ---\n",
    "# Name for the Google Sheet file in Drive (it will be a native Google Sheet)\n",
    "GSHEET_FILE_NAME = \"CB1_311_Complaint_Data_GSheet\" # No .xlsx extension needed for native Google Sheet\n",
    "GSHEET_MIMETYPE = \"application/vnd.google-apps.spreadsheet\"\n",
    "# This is the name of the file as it will appear in Google Drive\n",
    "GSHEET_DRIVE_FILE_NAME = \"CB1_311_Complaint_Data_GSheet\" # Renamed for clarity\n",
    "\n",
    "# --- Google Drive Configuration ---\n",
    "# IMPORTANT: Replace with the actual ID of your Google Drive folder\n",
    "# where you want to store the Google Sheet file.\n",
    "# To get this: Go to your Google Drive, create a new folder, open it,\n",
    "# the ID is in the URL: https://drive.google.com/drive/folders/YOUR_ID\n",
    "GOOGLE_DRIVE_FOLDER_ID = \"0AI4Egw2Y1IwhUk9PVA\" # <--- UPDATED\n",
    "\n",
    "# Service Account Key file name (downloaded from GCP)\n",
    "SERVICE_ACCOUNT_KEY_FILE = \"/content/drive/Shareddrives/311_Complaint_Data/service_account_key.json\"\n",
    "\n",
    "# 3. DEFINITIVE FIX: Set the start date to the PAST\n",
    "DEFAULT_INITIAL_FETCH_DATE = \"2025-04-01\"\n",
    "\n",
    "# --- Checkpointing Functions ---\n",
    "# For Colab, progress file should also be in Drive or a persistent location\n",
    "PROGRESS_FILE_NAME = \"last_processed_date.txt\" # Example for Colab persistent storage\n",
    "PROGRESS_FILE_MIMETYPE = \"text/plain\"\n",
    "\n",
    "def read_last_processed_date(drive_service) -> str:\n",
    "    \"\"\"Reads the last processed date from the progress file (in Google Drive).\"\"\"\n",
    "    print(\"Attempting to read last processed date from Google Drive...\")\n",
    "    try:\n",
    "        # Search for the progress file within the specified GOOGLE_DRIVE_FOLDER_ID\n",
    "        file_list_response = drive_service.files().list(\n",
    "            q=f\"'{GOOGLE_DRIVE_FOLDER_ID}' in parents and trashed=false and name='{PROGRESS_FILE_NAME}'\",\n",
    "            spaces='drive',\n",
    "            fields='files(id, name)'\n",
    "        ).execute()\n",
    "        file_items = file_list_response.get('files', [])\n",
    "\n",
    "\n",
    "        if file_items:\n",
    "            progress_file_id = file_items[0]['id']\n",
    "            print(f\"Found progress file in Drive (ID: {progress_file_id}). Downloading...\")\n",
    "            request = drive_service.files().get_media(fileId=progress_file_id)\n",
    "            file_content_bytes = io.BytesIO()\n",
    "            downloader = MediaIoBaseDownload(file_content_bytes, request)\n",
    "            done = False\n",
    "            while done is False:\n",
    "                status, done = downloader.next_chunk()\n",
    "                if status:\n",
    "                    print(f\"Download {int(status.progress() * 100)}%.\")\n",
    "            file_content_bytes.seek(0)\n",
    "\n",
    "            date_str = file_content_bytes.getvalue().decode('utf-8').strip()\n",
    "            try:\n",
    "                datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                print(f\"Resuming from last processed date: {date_str}\")\n",
    "                return date_str\n",
    "            except ValueError:\n",
    "                print(f\"Invalid date format in Drive progress file. Starting from default.\")\n",
    "                return DEFAULT_INITIAL_FETCH_DATE\n",
    "        print(f\"No progress file found in Drive. Starting from default date: {DEFAULT_INITIAL_FETCH_DATE}\")\n",
    "        return DEFAULT_INITIAL_FETCH_DATE\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading progress file from Drive: {e}. Starting from default.\")\n",
    "        return DEFAULT_INITIAL_FETCH_DATE\n",
    "\n",
    "\n",
    "def write_last_processed_date(date_str: str, drive_service):\n",
    "    \"\"\"Writes the last successfully processed date to the progress file (in Google Drive).\"\"\"\n",
    "    try:\n",
    "        progress_buffer = io.BytesIO(date_str.encode('utf-8'))\n",
    "\n",
    "        # Search for the progress file within the specified GOOGLE_DRIVE_FOLDER_ID\n",
    "        file_list_response = drive_service.files().list(\n",
    "            q=f\"'{GOOGLE_DRIVE_FOLDER_ID}' in parents and trashed=false and name='{PROGRESS_FILE_NAME}'\",\n",
    "            spaces='drive',\n",
    "            fields='files(id, name)'\n",
    "        ).execute()\n",
    "        file_items = file_list_response.get('files', [])\n",
    "\n",
    "\n",
    "        if file_items:\n",
    "            progress_file_id = file_items[0]['id']\n",
    "            media = MediaIoBaseUpload(progress_buffer, mimetype=PROGRESS_FILE_MIMETYPE, resumable=True)\n",
    "            drive_service.files().update(\n",
    "                fileId=progress_file_id,\n",
    "                media_body=media,\n",
    "                fields='id, name'\n",
    "            ).execute()\n",
    "            print(f\"Progress saved to Drive: Last processed date is now {date_str}\")\n",
    "        else:\n",
    "            # Create new file in the specified GOOGLE_DRIVE_FOLDER_ID\n",
    "            file_metadata = {\n",
    "                'name': PROGRESS_FILE_NAME,\n",
    "                'parents': [GOOGLE_DRIVE_FOLDER_ID], # Set parent folder\n",
    "                'mimeType': PROGRESS_FILE_MIMETYPE\n",
    "            }\n",
    "            media = MediaIoBaseUpload(progress_buffer, mimetype=PROGRESS_FILE_MIMETYPE, resumable=True)\n",
    "            drive_service.files().create(\n",
    "                body=file_metadata,\n",
    "                media_body=media,\n",
    "                supportsAllDrives=True, # Crucial for Shared Drives\n",
    "                fields='id, name'\n",
    "            ).execute()\n",
    "            print(f\"Progress file created and saved to Drive: Last processed date is now {date_str}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not save progress to Drive file: {e}\")\n",
    "\n",
    "# --- Helper Function to Fetch Data from NYC Open Data API with Pagination ---\n",
    "def fetch_nyc_data_incremental(start_date_str: str, end_date_str: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetches NYC 311 data. This version uses a simple date filter that is more reliable.\n",
    "    \"\"\"\n",
    "    all_fetched_dfs = []\n",
    "    offset = 0\n",
    "    more_data_available = True\n",
    "\n",
    "    # Build a simple WHERE clause for the date filter\n",
    "    if end_date_str:\n",
    "        where_clause = f\"created_date >= '{start_date_str}T00:00:00.000' AND created_date <= '{end_date_str}T23:59:59.999'\"\n",
    "        print(f\"Starting API fetch from {start_date_str} to {end_date_str}...\")\n",
    "    else:\n",
    "        where_clause = f\"created_date > '{start_date_str}T23:59:59.999'\"\n",
    "        print(f\"Starting API fetch from {start_date_str} to present...\")\n",
    "\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    while more_data_available:\n",
    "        # Use simple URL parameters instead of a complex $query\n",
    "        params = {\n",
    "            '$limit': API_LIMIT_PER_REQUEST,\n",
    "            '$offset': offset,\n",
    "            '$where': where_clause,\n",
    "            '$order': 'created_date ASC'\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            print(f\"Fetching page with offset: {offset}...\")\n",
    "            response = session.get(NYC_OPEN_DATA_RESOURCE_URL, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Check for empty response text before reading CSV\n",
    "            if not response.text.strip():\n",
    "                print(\"API returned an empty response. Ending fetch.\")\n",
    "                more_data_available = False\n",
    "                continue\n",
    "\n",
    "            page_df = pd.read_csv(io.StringIO(response.text))\n",
    "\n",
    "            if not page_df.empty:\n",
    "                all_fetched_dfs.append(page_df)\n",
    "                offset += len(page_df) # Increment offset by records received\n",
    "                if len(page_df) < API_LIMIT_PER_REQUEST:\n",
    "                    more_data_available = False # Last page\n",
    "            else:\n",
    "                more_data_available = False\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data from API: {e}\")\n",
    "            break # Exit loop on error\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(\"No more data to process (Empty CSV).\")\n",
    "            more_data_available = False\n",
    "\n",
    "    if not all_fetched_dfs:\n",
    "        print(\"No new records found from API.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.concat(all_fetched_dfs, ignore_index=True)\n",
    "    print(f\"Total records fetched from API (before filtering): {len(df)}\")\n",
    "    return df\n",
    "\n",
    "# --- Main Function to Update Google Sheet and Upload to Google Drive ---\n",
    "def update_google_sheet_data():\n",
    "    \"\"\"\n",
    "    Main function to update the Google Sheet file with new data and upload it to Google Drive.\n",
    "    This function handles both initial historical load (chunked by year) and daily incremental updates.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Google Sheet Update and Google Drive Upload Process (Pure In-Memory) ---\")\n",
    "\n",
    "\n",
    "    # --- Step 0: Authenticate with Google Drive (Service Account) ---\n",
    "    print(\"Authenticating with Google Drive...\")\n",
    "    try:\n",
    "        # Mount Drive if in Colab (only if not already mounted)\n",
    "        if COLAB_ENV and not os.path.exists('/content/drive'):\n",
    "            drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "        creds = service_account.Credentials.from_service_account_file(\n",
    "            SERVICE_ACCOUNT_KEY_FILE,\n",
    "            scopes=['https://www.googleapis.com/auth/drive.file']\n",
    "        )\n",
    "        drive_service = build('drive', 'v3', credentials=creds)\n",
    "        print(\"Google Drive authentication successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Google Drive authentication failed: {e}\")\n",
    "        print(f\"Please ensure '{SERVICE_ACCOUNT_KEY_FILE}' is in the correct Google Drive folder and accessible via mounting.\")\n",
    "        print(f\"Also check Service Account permissions in Google Cloud Console and Drive folder sharing.\")\n",
    "        return # Exit if authentication fails\n",
    "\n",
    "\n",
    "    # --- Step 1: Download existing Google Sheet from Google Drive (if exists) ---\n",
    "    existing_df = pd.DataFrame()\n",
    "    last_created_date_in_drive = None\n",
    "\n",
    "    # Search for the file in the specified folder\n",
    "    file_id_in_drive = None\n",
    "    try:\n",
    "        file_list_response = drive_service.files().list(\n",
    "            q=f\"'{GOOGLE_DRIVE_FOLDER_ID}' in parents and trashed=false and name='{GSHEET_DRIVE_FILE_NAME}'\",\n",
    "            spaces='drive',\n",
    "            fields='files(id, name, mimeType)' # Request mimeType to check if it's a native GSheet\n",
    "        ).execute()\n",
    "        file_items = file_list_response.get('files', [])\n",
    "\n",
    "\n",
    "        if file_items:\n",
    "            file_id_in_drive = file_items[0]['id']\n",
    "            print(f\"Found existing Google Sheet (ID: {file_id_in_drive}). Downloading...\")\n",
    "\n",
    "            # Download the file content into a BytesIO object\n",
    "            # For native Google Sheets, you must export them as CSV to read with pandas\n",
    "            if file_items[0]['mimeType'] == GSHEET_MIMETYPE:\n",
    "                request = drive_service.files().export_media(fileId=file_id_in_drive, mimeType='text/csv')\n",
    "            else: # If it's a regular CSV file (e.g., uploaded directly)\n",
    "                request = drive_service.files().get_media(fileId=file_id_in_drive)\n",
    "\n",
    "\n",
    "            file_content_bytes = io.BytesIO()\n",
    "            downloader = MediaIoBaseDownload(file_content_bytes, request)\n",
    "            done = False\n",
    "            while done is False:\n",
    "                status, done = downloader.next_chunk()\n",
    "                if status:\n",
    "                    print(f\"Download {int(status.progress() * 100)}%.\")\n",
    "            file_content_bytes.seek(0)\n",
    "\n",
    "            try:\n",
    "                # Read the BytesIO object directly into a DataFrame\n",
    "                existing_df = pd.read_csv(file_content_bytes)\n",
    "                if 'created_date' in existing_df.columns and not existing_df.empty:\n",
    "                    existing_df['created_date'] = pd.to_datetime(existing_df['created_date'], errors='coerce')\n",
    "                    last_created_date_in_drive = existing_df['created_date'].max()\n",
    "                    print(f\"Latest created_date found in Google Drive Google Sheet: {last_created_date_in_drive}\")\n",
    "                else:\n",
    "                    print(\"Google Drive Google Sheet is empty or 'created_date' column is missing. Starting from default date.\")\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Google Drive Google Sheet '{GSHEET_DRIVE_FILE_NAME}' is empty. Will create a new one.\")\n",
    "                existing_df = pd.DataFrame()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading existing Google Sheet from Google Drive: {e}. Starting with empty DataFrame.\")\n",
    "                existing_df = pd.DataFrame()\n",
    "        else:\n",
    "            print(\"Google Sheet not found in Google Drive. Will create a new one.\")\n",
    "    except HttpError as error:\n",
    "        print(f\"An HTTP error occurred while checking/downloading from Drive: {error}\")\n",
    "        print(\"Assuming no existing file in Drive for now.\")\n",
    "        file_id_in_drive = None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Drive file check/download: {e}\")\n",
    "        print(\"Assuming no existing file in Drive for now.\")\n",
    "        file_id_in_drive = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Determine the start date for fetching new data\n",
    "    if last_created_date_in_drive: # Use date from Drive if available\n",
    "        fetch_start_date = (last_created_date_in_drive + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    else: # Otherwise, use checkpoint or default\n",
    "        last_processed_date_str = read_last_processed_date(drive_service) # Pass drive_service\n",
    "        fetch_start_date = (datetime.strptime(last_processed_date_str, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    # If fetch_start_date is before DEFAULT_INITIAL_FETCH_DATE, clamp it\n",
    "    if datetime.strptime(fetch_start_date, '%Y-%m-%d') < datetime.strptime(DEFAULT_INITIAL_FETCH_DATE, '%Y-%m-%d'):\n",
    "        fetch_start_date = DEFAULT_INITIAL_FETCH_DATE\n",
    "        print(f\"Adjusted fetch start date to DEFAULT_INITIAL_FETCH_DATE: {fetch_start_date}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Determining data fetch strategy. Actual fetch start date: {fetch_start_date}\")\n",
    "\n",
    "\n",
    "    # --- Step 3: Fetch Data (Historical Chunks or Incremental) ---\n",
    "    all_fetched_data_frames = []\n",
    "\n",
    "\n",
    "    # This logic now primarily handles the initial full load or large gaps\n",
    "    # If the Google Sheet is empty in Drive OR the last processed date is significantly old (e.g., from a previous year)\n",
    "    # OR if the checkpoint date is very old, perform historical chunking.\n",
    "    current_year = datetime.now().year\n",
    "    start_year_for_chunking = datetime.strptime(fetch_start_date, '%Y-%m-%d').year\n",
    "\n",
    "\n",
    "    # Only perform chunking if we're starting from a historical year (not just a few days ago in current year)\n",
    "    # And if we haven't completed the historical load yet.\n",
    "    # The condition `not existing_df.empty` here is crucial: if existing_df is empty, it means Drive file was not found or empty\n",
    "    # so we need to do a full historical pull.\n",
    "    if not existing_df.empty and start_year_for_chunking < current_year: # Only chunk if existing_df is NOT empty and we are in a historical year\n",
    "        print(\"Performing historical data fetch in yearly chunks...\")\n",
    "        for year in range(start_year_for_chunking, current_year + 1):\n",
    "            year_start_date = f\"{year}-01-01\"\n",
    "            year_end_date = f\"{year}-12-31\"\n",
    "\n",
    "            if year == current_year:\n",
    "                year_end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                if datetime.strptime(year_start_date, '%Y-%m-%d') > datetime.strptime(year_end_date, '%Y-%m-%d'):\n",
    "                    continue\n",
    "\n",
    "\n",
    "            print(f\"\\n--- Fetching data for year: {year} ({year_start_date} to {year_end_date}) ---\")\n",
    "\n",
    "            # Only fetch if the year's start date is relevant to the overall fetch_start_date\n",
    "            if datetime.strptime(year_end_date, '%Y-%m-%d') >= datetime.strptime(fetch_start_date, '%Y-%m-%d'):\n",
    "                # Adjust year_start_date if it's earlier than fetch_start_date (for first relevant chunk)\n",
    "                actual_chunk_start_date = max(datetime.strptime(year_start_date, '%Y-%m-%d'), datetime.strptime(fetch_start_date, '%Y-%m-%d')).strftime('%Y-%m-%d')\n",
    "\n",
    "                chunk_df = fetch_nyc_data_incremental(actual_chunk_start_date, year_end_date)\n",
    "                if not chunk_df.empty:\n",
    "                    all_fetched_data_frames.append(chunk_df)\n",
    "                else:\n",
    "                    print(f\"No data found for year {year} or fetch failed.\")\n",
    "            else:\n",
    "                print(f\"Skipping year {year} as it's older than the required fetch_start_date ({fetch_start_date}).\")\n",
    "\n",
    "\n",
    "        if not all_fetched_data_frames:\n",
    "            print(\"No new data fetched during historical chunking. Google Sheet will not be updated.\")\n",
    "            return\n",
    "\n",
    "        new_data_df = pd.concat(all_fetched_data_frames, ignore_index=True)\n",
    "        print(f\"Total records fetched during historical chunking: {len(new_data_df)}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"Performing daily incremental data fetch from: {fetch_start_date}...\")\n",
    "        new_data_df = fetch_nyc_data_incremental(fetch_start_date)\n",
    "\n",
    "        if not new_data_df.empty:\n",
    "            print(\"Filtering data in Python...\")\n",
    "\n",
    "            # 1. Filter for the correct community board\n",
    "            # The .str.contains() is a flexible way to find the board\n",
    "            filtered_df = new_data_df[new_data_df['community_board'].str.contains('01 MANHATTAN', na=False)].copy()\n",
    "            print(f\"Rows after community board filter: {len(filtered_df)}\")\n",
    "\n",
    "            # 2. Filter for the correct zip codes\n",
    "            # The .isin() method is the Pandas equivalent of SQL's IN\n",
    "            filtered_df = filtered_df[filtered_df['incident_zip'].isin(ZIP_CODES_TO_INCLUDE)].copy()\n",
    "            print(f\"Rows after zip code filter: {len(filtered_df)}\")\n",
    "\n",
    "            new_data_df = filtered_df # Assign the final filtered data\n",
    "        if new_data_df.empty:\n",
    "            print(\"No new data fetched from API for incremental update. Google Sheet will not be updated.\")\n",
    "            return\n",
    "\n",
    "\n",
    "    # --- Step 3: Combine existing data with new data ---\n",
    "    # Ensure column order is consistent before concat\n",
    "    if not existing_df.empty:\n",
    "        new_data_df = new_data_df.reindex(columns=existing_df.columns, fill_value=None)\n",
    "        combined_df = pd.concat([existing_df, new_data_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_data_df\n",
    "\n",
    "    # Optional: Remove potential duplicates if 'unique_key' exists\n",
    "    if 'unique_key' in combined_df.columns:\n",
    "        initial_rows = len(combined_df)\n",
    "        combined_df.drop_duplicates(subset=['unique_key'], inplace=True, keep='last')\n",
    "        if len(combined_df) < initial_rows:\n",
    "            print(f\"Removed {initial_rows - len(combined_df)} duplicate rows based on 'unique_key'.\")\n",
    "\n",
    "\n",
    "    print(f\"Total rows in combined DataFrame: {len(combined_df)}\")\n",
    "\n",
    "\n",
    "    # --- Step 4: Prepare CSV content in memory for upload ---\n",
    "    csv_buffer = io.StringIO()\n",
    "    # Write to CSV in-memory, then encode to bytes\n",
    "    combined_df.to_csv(csv_buffer, index=False, encoding='utf-8')\n",
    "    csv_buffer.seek(0) # Rewind to the beginning of the buffer\n",
    "\n",
    "\n",
    "    # --- Step 5: Upload/Update Google Sheet file to Google Drive ---\n",
    "    print(\"Uploading/Updating Google Sheet file to Google Drive...\")\n",
    "\n",
    "    # Use file_id_in_drive to determine if update or create\n",
    "    if file_id_in_drive: # File was found and downloaded in Step 1\n",
    "        # Update existing file\n",
    "        file_metadata = {'name': GSHEET_FILE_NAME} # Name is sufficient for update\n",
    "\n",
    "        # Use MediaIoBaseUpload for in-memory content (CSV bytes)\n",
    "        media = MediaIoBaseUpload(\n",
    "            io.BytesIO(csv_buffer.getvalue().encode('utf-8')), # Pass the BytesIO object directly\n",
    "            mimetype='text/csv', # Source MIME type is CSV\n",
    "            resumable=True\n",
    "        )\n",
    "\n",
    "\n",
    "        try: # Added try-except for the update operation\n",
    "            updated_file = drive_service.files().update(\n",
    "                fileId=file_id_in_drive, # Use the ID of the found file\n",
    "                body=file_metadata, # Pass file_metadata as body\n",
    "                media_body=media,\n",
    "                supportsAllDrives=True, # Crucial for Shared Drives\n",
    "                fields='id, name'\n",
    "            ).execute()\n",
    "            print(f\"Updated existing Google Sheet on Google Drive (ID: {updated_file.get('id')}).\")\n",
    "            # Write checkpoint after successful upload\n",
    "            write_last_processed_date(datetime.now().strftime('%Y-%m-%d'), drive_service)\n",
    "        except HttpError as update_error:\n",
    "            if update_error.resp.status == 404:\n",
    "                print(f\"Warning: Google Sheet with ID {file_id_in_drive} not found during update. Attempting to create a new one instead.\")\n",
    "                file_id_in_drive = None # Reset ID so it falls into the 'else' block for creation\n",
    "            else:\n",
    "                raise # Re-raise other HttpErrors\n",
    "\n",
    "\n",
    "    if not file_id_in_drive: # If file was not found initially, or 404 on update\n",
    "        # Upload new file (as a native Google Sheet)\n",
    "        file_metadata = {\n",
    "            'name': GSHEET_FILE_NAME,\n",
    "            'parents': [GOOGLE_DRIVE_FOLDER_ID],\n",
    "            'mimeType': GSHEET_MIMETYPE # Set MIME type for native Google Sheet\n",
    "        }\n",
    "\n",
    "        # Use MediaIoBaseUpload for in-memory content (CSV bytes)\n",
    "        media = MediaIoBaseUpload(\n",
    "            io.BytesIO(csv_buffer.getvalue().encode('utf-8')), # Pass the BytesIO object directly\n",
    "            mimetype='text/csv', # Source MIME type is CSV\n",
    "            resumable=True\n",
    "        )\n",
    "\n",
    "        # Create a new file, Google Drive will convert the CSV to a native Google Sheet\n",
    "        new_file = drive_service.files().create(\n",
    "            body=file_metadata,\n",
    "            media_body=media,\n",
    "            supportsAllDrives=True, # Crucial for Shared Drives\n",
    "            fields='id, name'\n",
    "        ).execute()\n",
    "        print(f\"Uploaded new Google Sheet to Google Drive (ID: {new_file.get('id')}).\")\n",
    "        # Write checkpoint after successful upload (for new file creation)\n",
    "        write_last_processed_date(datetime.now().strftime('%Y-%m-%d'), drive_service)\n",
    "\n",
    "\n",
    "    print(\"--- Google Sheet Update and Google Drive Upload Process Completed ---\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    update_google_sheet_data()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOeEtvHkfi5I3CyTRGDcB8h",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
