{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLgkctneICAuXrml1pWgmp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiquynhdang03/Dang-Quynh-Chi/blob/main/gg_sheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgDnNxjzRNL8",
        "outputId": "93cc3748-0a8c-49b9-cb4c-bf06d1c8c72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--- Starting Google Sheet Update and Google Drive Upload Process (Pure In-Memory) ---\n",
            "Authenticating with Google Drive...\n",
            "Google Drive authentication successful.\n",
            "Google Sheet not found in Google Drive. Will create a new one.\n",
            "No progress file found in Drive. Starting from default date: 2025-01-01\n",
            "Determining data fetch strategy. Actual fetch start date: 2025-01-02\n",
            "Performing daily incremental data fetch from: 2025-01-02...\n",
            "Starting API fetch from 2025-01-02 to present...\n",
            "Fetching page with offset: 0. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 0.\n",
            "Fetching page with offset: 1000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 1000.\n",
            "Fetching page with offset: 2000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 2000.\n",
            "Fetching page with offset: 3000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 3000.\n",
            "Fetching page with offset: 4000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 4000.\n",
            "Fetching page with offset: 5000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 5000.\n",
            "Fetching page with offset: 6000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=9, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='data.cityofnewyork.us', port=443): Read timed out. (read timeout=60)\")': /resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20complaint_type%2C%20descriptor%2C%20location_type%2C%20incident_zip%2C%20incident_address%2C%20street_name%2C%20cross_street_1%2C%20cross_street_2%2C%20intersection_street_1%2C%20intersection_street_2%2C%20address_type%2C%20city%2C%20landmark%2C%20facility_type%2C%20status%2C%20due_date%2C%20resolution_description%2C%20resolution_action_updated_date%2C%20community_board%2C%20bbl%2C%20borough%2C%20x_coordinate_state_plane%2C%20y_coordinate_state_plane%2C%20open_data_channel_type%2C%20park_facility_name%2C%20park_borough%2C%20vehicle_type%2C%20taxi_company_borough%2C%20taxi_pick_up_location%2C%20bridge_highway_name%2C%20bridge_highway_direction%2C%20road_ramp%2C%20bridge_highway_segment%2C%20latitude%2C%20longitude%2C%20location%20WHERE%20%28created_date%20%3E%20%272025-01-02%27%29%20AND%20created_date%20IS%20NOT%20NULL%20AND%20%28upper%28%60community_board%60%29%20LIKE%20%27%2501%20MANHATTAN%25%27%29%20ORDER%20BY%20created_date%20ASC%20LIMIT%201000%20OFFSET%206000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully fetched 1000 records for offset 6000.\n",
            "Fetching page with offset: 7000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 7000.\n",
            "Fetching page with offset: 8000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 8000.\n",
            "Fetching page with offset: 9000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 9000.\n",
            "Fetching page with offset: 10000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 10000.\n",
            "Fetching page with offset: 11000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 11000.\n",
            "Fetching page with offset: 12000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 12000.\n",
            "Fetching page with offset: 13000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 13000.\n",
            "Fetching page with offset: 14000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 14000.\n",
            "Fetching page with offset: 15000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 15000.\n",
            "Fetching page with offset: 16000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 16000.\n",
            "Fetching page with offset: 17000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 17000.\n",
            "Fetching page with offset: 18000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 1000 records for offset 18000.\n",
            "Fetching page with offset: 19000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 132 records for offset 19000.\n",
            "Fetching page with offset: 20000. URL: https://data.cityofnewyork.us/resource/erm2-nwe9.csv?$query=SELECT%20unique_key%2C%20created_date%2C%20closed_date%2C%20agency%2C%20agency_name%2C%20c...\n",
            "Successfully fetched 0 records for offset 20000.\n",
            "Total records fetched and processed: 19132\n",
            "Total rows in combined DataFrame: 19132\n",
            "Uploading/Updating Google Sheet file to Google Drive...\n",
            "Uploaded new Google Sheet to Google Drive (ID: 18qGhziWt6e5mImmHXL9wTFdzzSS8e9KZntdHNww7B7Y).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not save progress to Drive file: <HttpError 403 when requesting https://www.googleapis.com/upload/drive/v3/files?fields=id%2C+name&alt=json&uploadType=resumable returned \"Service Accounts do not have storage quota. Leverage shared drives (https://developers.google.com/workspace/drive/api/guides/about-shareddrives), or use OAuth delegation (http://support.google.com/a/answer/7281227) instead.\". Details: \"[{'message': 'Service Accounts do not have storage quota. Leverage shared drives (https://developers.google.com/workspace/drive/api/guides/about-shareddrives), or use OAuth delegation (http://support.google.com/a/answer/7281227) instead.', 'domain': 'usageLimits', 'reason': 'storageQuotaExceeded'}]\">\n",
            "--- Google Sheet Update and Google Drive Upload Process Completed ---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!pip install google-auth google-api-python-client urllib3 --upgrade\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import time # Import time for sleep functionality\n",
        "import io # Import io for string stream stream handling\n",
        "\n",
        "\n",
        "# NEW IMPORTS for direct Google API client authentication\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownload\n",
        "from googleapiclient.errors import HttpError # Import HttpError for specific error handling\n",
        "\n",
        "\n",
        "# --- Configuration Constants ---\n",
        "# Changed to CSV endpoint\n",
        "NYC_OPEN_DATA_RESOURCE_URL = \"https://data.cityofnewyork.us/resource/erm2-nwe9.csv\"\n",
        "API_LIMIT_PER_REQUEST = 1000\n",
        "COMMUNITY_BOARD_FILTER = \"upper(`community_board`) LIKE '%01 MANHATTAN%'\"\n",
        "\n",
        "\n",
        "# --- Google Sheet File Configuration ---\n",
        "# Name for the Google Sheet file in Drive (it will be a native Google Sheet)\n",
        "GSHEET_FILE_NAME = \"CB1_311_Complaint_Data_GSheet\" # No .xlsx extension needed for native Google Sheet\n",
        "GSHEET_MIMETYPE = \"application/vnd.google-apps.spreadsheet\"\n",
        "# This is the name of the file as it will appear in Google Drive\n",
        "GSHEET_DRIVE_FILE_NAME = \"CB1_311_Complaint_Data_GSheet\" # Renamed for clarity\n",
        "\n",
        "\n",
        "# --- Google Drive Configuration ---\n",
        "GOOGLE_DRIVE_FOLDER_ID = \"0AI4Egw2Y1IwhUk9PVA\"\n",
        "\n",
        "\n",
        "# Service Account Key file name (downloaded from GCP)\n",
        "SERVICE_ACCOUNT_KEY_FILE = \"/content/drive/Shareddrives/311_Complaint_Data/service_account_key.json\"\n",
        "\n",
        "\n",
        "# Default start date for initial data fetch if Google Sheet doesn't exist or is empty\n",
        "# This should align with your historical data start (e.g., 2018-07-01)\n",
        "DEFAULT_INITIAL_FETCH_DATE = \"2025-01-01\"\n",
        "\n",
        "\n",
        "# --- Checkpointing Functions ---\n",
        "# For Colab, progress file should also be in Drive or a persistent location\n",
        "PROGRESS_FILE = \"/content/drive/MyDrive/Colab_Secrets/last_processed_date.txt\" # Example for Colab persistent storage\n",
        "\n",
        "\n",
        "def read_last_processed_date() -> str:\n",
        "    \"\"\"Reads the last processed date from the progress file (in Drive for Colab).\"\"\"\n",
        "    # Authenticate to Drive to read progress file\n",
        "    try:\n",
        "        creds = service_account.Credentials.from_service_account_file(\n",
        "            SERVICE_ACCOUNT_KEY_FILE,\n",
        "            scopes=['https://www.googleapis.com/auth/drive.file']\n",
        "        )\n",
        "        drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "\n",
        "        # Search for the progress file\n",
        "        file_list_response = drive_service.files().list(\n",
        "            q=f\"name='{os.path.basename(PROGRESS_FILE)}' and trashed=false\", # Search by name, not folder ID\n",
        "            spaces='drive',\n",
        "            fields='files(id, name)'\n",
        "        ).execute()\n",
        "        file_items = file_list_response.get('files', [])\n",
        "\n",
        "\n",
        "        if file_items:\n",
        "            progress_file_id = file_items[0]['id']\n",
        "            print(f\"Found progress file in Drive (ID: {progress_file_id}). Downloading...\")\n",
        "            request = drive_service.files().get_media(fileId=progress_file_id)\n",
        "            file_content_bytes = io.BytesIO()\n",
        "            downloader = MediaIoBaseDownload(file_content_bytes, request)\n",
        "            done = False\n",
        "            while done is False:\n",
        "                status, done = downloader.next_chunk()\n",
        "            file_content_bytes.seek(0)\n",
        "\n",
        "\n",
        "            date_str = file_content_bytes.getvalue().decode('utf-8').strip()\n",
        "            try:\n",
        "                datetime.strptime(date_str, '%Y-%m-%d')\n",
        "                print(f\"Resuming from last processed date: {date_str}\")\n",
        "                return date_str\n",
        "            except ValueError:\n",
        "                print(f\"Invalid date format in Drive progress file. Starting from default.\")\n",
        "                return DEFAULT_INITIAL_FETCH_DATE\n",
        "        print(f\"No progress file found in Drive. Starting from default date: {DEFAULT_INITIAL_FETCH_DATE}\")\n",
        "        return DEFAULT_INITIAL_FETCH_DATE\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading progress file from Drive: {e}. Starting from default.\")\n",
        "        return DEFAULT_INITIAL_FETCH_DATE\n",
        "\n",
        "\n",
        "def write_last_processed_date(date_str: str, drive_service):\n",
        "    \"\"\"Writes the last successfully processed date to the progress file (in Drive).\"\"\"\n",
        "    try:\n",
        "        # Prepare content in memory\n",
        "        progress_buffer = io.BytesIO(date_str.encode('utf-8'))\n",
        "\n",
        "\n",
        "        # Check if progress file exists in Drive\n",
        "        file_list_response = drive_service.files().list(\n",
        "            q=f\"name='{os.path.basename(PROGRESS_FILE)}' and trashed=false\",\n",
        "            spaces='drive',\n",
        "            fields='files(id, name)'\n",
        "        ).execute()\n",
        "        file_items = file_list_response.get('files', [])\n",
        "\n",
        "\n",
        "        if file_items:\n",
        "            progress_file_id = file_items[0]['id']\n",
        "            # Update existing file\n",
        "            media = MediaIoBaseUpload(progress_buffer, mimetype='text/plain', resumable=True)\n",
        "            drive_service.files().update(\n",
        "                fileId=progress_file_id,\n",
        "                media_body=media,\n",
        "                fields='id, name'\n",
        "            ).execute()\n",
        "            print(f\"Progress saved to Drive: Last processed date is now {date_str}\")\n",
        "        else:\n",
        "            # Create new file\n",
        "            file_metadata = {\n",
        "                'name': os.path.basename(PROGRESS_FILE),\n",
        "                'mimeType': 'text/plain'\n",
        "            }\n",
        "            media = MediaIoBaseUpload(progress_buffer, mimetype='text/plain', resumable=True)\n",
        "            drive_service.files().create(\n",
        "                body=file_metadata,\n",
        "                media_body=media,\n",
        "                fields='id, name'\n",
        "            ).execute()\n",
        "            print(f\"Progress file created and saved to Drive: Last processed date is now {date_str}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not save progress to Drive file: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Helper Function to Fetch Data from NYC Open Data API with Pagination ---\n",
        "def fetch_nyc_data_incremental(start_date_str: str, end_date_str: str = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetches NYC 311 data incrementally from the API using pagination.\n",
        "    Can fetch for a specific range (start_date_str to end_date_str) or from start_date_str to present.\n",
        "    Args:\n",
        "        start_date_str: The date string (YYYY-MM-DD) from which to start fetching data.\n",
        "        end_date_str: Optional. The date string (YYYY-MM-DD) to end fetching data. If None, fetches to present.\n",
        "    Returns:\n",
        "        A pandas DataFrame containing the fetched data.\n",
        "    \"\"\"\n",
        "    all_fetched_dfs = [] # Changed to store DataFrames\n",
        "    offset = 0\n",
        "    more_data_available = True\n",
        "    headers = None # To store headers from the first page\n",
        "\n",
        "\n",
        "    date_filter_clause = \"\"\n",
        "    if end_date_str:\n",
        "        date_filter_clause = f\"(created_date BETWEEN '{start_date_str}' AND '{end_date_str}') AND created_date IS NOT NULL\"\n",
        "        print(f\"Starting API fetch from {start_date_str} to {end_date_str}...\")\n",
        "    else:\n",
        "        date_filter_clause = f\"(created_date > '{start_date_str}') AND created_date IS NOT NULL\"\n",
        "        print(f\"Starting API fetch from {start_date_str} to present...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Base SoQL query for selecting columns and filtering by community board\n",
        "    soql_base_query = (\n",
        "        \"SELECT unique_key, created_date, closed_date, agency, agency_name, \"\n",
        "        \"complaint_type, descriptor, location_type, incident_zip, incident_address, \"\n",
        "        \"street_name, cross_street_1, cross_street_2, intersection_street_1, intersection_street_2, \"\n",
        "        \"address_type, city, landmark, facility_type, status, due_date, resolution_description, \"\n",
        "        \"resolution_action_updated_date, community_board, bbl, borough, x_coordinate_state_plane, \"\n",
        "        \"y_coordinate_state_plane, open_data_channel_type, park_facility_name, park_borough, \"\n",
        "        \"vehicle_type, taxi_company_borough, taxi_pick_up_location, bridge_highway_name, \"\n",
        "        \"bridge_highway_direction, road_ramp, bridge_highway_segment, latitude, longitude, location \"\n",
        "        f\"WHERE {date_filter_clause} AND ({COMMUNITY_BOARD_FILTER}) \"\n",
        "        \"ORDER BY created_date ASC\" # Use ASC for incremental load to avoid missing data if API order changes\n",
        "    )\n",
        "\n",
        "\n",
        "    # Configure requests session with retries and backoff\n",
        "    session = requests.Session()\n",
        "    # Increased backoff_factor to wait longer between retries\n",
        "    retries = Retry(total=10, backoff_factor=2, status_forcelist=[ 500, 502, 503, 504, 429 ]) # Added 429 to retry list\n",
        "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "\n",
        "    try:\n",
        "        while more_data_available:\n",
        "            full_paginated_soql_query = f\"{soql_base_query} LIMIT {API_LIMIT_PER_REQUEST} OFFSET {offset}\"\n",
        "            encoded_soql_query = requests.utils.quote(full_paginated_soql_query)\n",
        "            paginated_api_url = f\"{NYC_OPEN_DATA_RESOURCE_URL}?$query={encoded_soql_query}\"\n",
        "\n",
        "\n",
        "            print(f\"Fetching page with offset: {offset}. URL: {paginated_api_url[:150]}...\") # Truncate URL for log\n",
        "\n",
        "\n",
        "            # Increased timeout for the request itself\n",
        "            response = session.get(paginated_api_url, timeout=60) # Increased timeout to 60 seconds\n",
        "            response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "\n",
        "\n",
        "            # Read CSV data directly into a DataFrame\n",
        "            # Use io.StringIO to treat the response text as a file\n",
        "            page_df = pd.read_csv(io.StringIO(response.text))\n",
        "\n",
        "\n",
        "            print(f\"Successfully fetched {len(page_df)} records for offset {offset}.\")\n",
        "\n",
        "\n",
        "            if not page_df.empty:\n",
        "                if offset == 0:\n",
        "                    # Store headers from the first page\n",
        "                    headers = page_df.columns.tolist()\n",
        "                    all_fetched_dfs.append(page_df)\n",
        "                else:\n",
        "                    # For subsequent pages, ensure column order matches the first page's headers\n",
        "                    # and append only data rows (excluding potential duplicate headers in CSV)\n",
        "                    if not headers: # Should not happen if offset 0 processed correctly\n",
        "                        headers = page_df.columns.tolist() # Fallback\n",
        "                    page_df = page_df[page_df.columns.intersection(headers)] # Keep only common columns\n",
        "                    page_df = page_df.reindex(columns=headers) # Reorder to match initial headers\n",
        "                    all_fetched_dfs.append(page_df)\n",
        "\n",
        "\n",
        "                offset += API_LIMIT_PER_REQUEST\n",
        "            else:\n",
        "                more_data_available = False # No more data to fetch\n",
        "\n",
        "\n",
        "            # Implement a small delay to respect API rate limits\n",
        "            time.sleep(5) # Increased sleep to 5 seconds\n",
        "\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching data from API: {e}\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"No more data for offset {offset} (Empty CSV response).\")\n",
        "        more_data_available = False\n",
        "    except Exception as e: # Catch other potential errors during CSV parsing\n",
        "        print(f\"Error parsing CSV response: {e}. Response content: {response.text[:200] if response else 'No response text'}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "    if not all_fetched_dfs:\n",
        "        print(\"No new records found from API.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Concatenate all DataFrames into one\n",
        "    df = pd.concat(all_fetched_dfs, ignore_index=True)\n",
        "\n",
        "\n",
        "    # Convert date columns to datetime objects\n",
        "    date_cols = ['created_date', 'closed_date', 'due_date', 'resolution_action_updated_date']\n",
        "    for col in date_cols:\n",
        "        if col in df.columns:\n",
        "            # Handle potential mixed types or errors by coercing to NaT (Not a Time)\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
        "            # Convert to local timezone if needed, then format\n",
        "            df[col] = df[col].dt.tz_convert('America/New_York').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "\n",
        "    print(f\"Total records fetched and processed: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# --- Main Function to Update CSV and Upload to Google Drive ---\n",
        "def update_csv_and_upload_to_drive():\n",
        "    \"\"\"\n",
        "    Main function to update the local CSV file with new data and upload it to Google Drive.\n",
        "    This function handles both initial historical load (chunked by year) and daily incremental updates.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Google Sheet Update and Google Drive Upload Process (Pure In-Memory) ---\")\n",
        "\n",
        "\n",
        "    # --- Step 0: Authenticate with Google Drive (Service Account) ---\n",
        "    print(\"Authenticating with Google Drive...\")\n",
        "    try:\n",
        "        creds = service_account.Credentials.from_service_account_file(\n",
        "            SERVICE_ACCOUNT_KEY_FILE,\n",
        "            scopes=['https://www.googleapis.com/auth/drive.file']\n",
        "        )\n",
        "        drive_service = build('drive', 'v3', credentials=creds)\n",
        "        print(\"Google Drive authentication successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Google Drive authentication failed: {e}\")\n",
        "        print(f\"Please ensure '{SERVICE_ACCOUNT_KEY_FILE}' is in the correct Google Drive folder and accessible via mounting.\")\n",
        "        print(f\"Also check Service Account permissions in Google Cloud Console and Drive folder sharing.\")\n",
        "        return # Exit if authentication fails\n",
        "\n",
        "\n",
        "    # --- Step 1: Download existing Google Sheet from Google Drive (if exists) ---\n",
        "    existing_df = pd.DataFrame()\n",
        "    last_created_date_in_drive = None # Renamed for clarity\n",
        "\n",
        "\n",
        "    # Search for the file in the specified folder\n",
        "    file_id_in_drive = None\n",
        "    try:\n",
        "        file_list_response = drive_service.files().list(\n",
        "            q=f\"'{GOOGLE_DRIVE_FOLDER_ID}' in parents and trashed=false and name='{GSHEET_FILE_NAME}'\",\n",
        "            spaces='drive',\n",
        "            fields='files(id, name, mimeType)' # Request mimeType to check if it's a native GSheet\n",
        "        ).execute()\n",
        "        file_items = file_list_response.get('files', [])\n",
        "\n",
        "\n",
        "        if file_items:\n",
        "            file_id_in_drive = file_items[0]['id']\n",
        "            print(f\"Found existing Google Sheet (ID: {file_id_in_drive}). Downloading...\")\n",
        "\n",
        "\n",
        "            # Download the file content into a BytesIO object\n",
        "            # For native Google Sheets, you must export them as CSV to read with pandas\n",
        "            if file_items[0]['mimeType'] == GSHEET_MIMETYPE:\n",
        "                request = drive_service.files().export_media(fileId=file_id_in_drive, mimeType='text/csv')\n",
        "            else: # If it's a regular CSV file (e.g., uploaded directly)\n",
        "                request = drive_service.files().get_media(fileId=file_id_in_drive)\n",
        "\n",
        "\n",
        "            file_content_bytes = io.BytesIO() # Create an empty BytesIO object\n",
        "            downloader = MediaIoBaseDownload(file_content_bytes, request) # Use MediaIoBaseDownload\n",
        "            done = False\n",
        "            while done is False:\n",
        "                status, done = downloader.next_chunk()\n",
        "                if status:\n",
        "                    print(f\"Download {int(status.progress() * 100)}%.\")\n",
        "            file_content_bytes.seek(0) # Rewind to the beginning of the buffer\n",
        "\n",
        "\n",
        "            try:\n",
        "                # Read the BytesIO object directly into a DataFrame\n",
        "                existing_df = pd.read_csv(file_content_bytes)\n",
        "                if 'created_date' in existing_df.columns and not existing_df.empty:\n",
        "                    existing_df['created_date'] = pd.to_datetime(existing_df['created_date'], errors='coerce')\n",
        "                    last_created_date_in_drive = existing_df['created_date'].max() # Use this for checkpointing\n",
        "                    print(f\"Latest created_date found in Google Drive Google Sheet: {last_created_date_in_drive}\")\n",
        "                else:\n",
        "                    print(\"Google Drive Google Sheet is empty or 'created_date' column is missing. Starting from default date.\")\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Google Drive Google Sheet '{GSHEET_FILE_NAME}' is empty. Will create a new one.\")\n",
        "                existing_df = pd.DataFrame()\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading existing Google Sheet from Google Drive: {e}. Starting with empty DataFrame.\")\n",
        "                existing_df = pd.DataFrame()\n",
        "        else:\n",
        "            print(\"Google Sheet not found in Google Drive. Will create a new one.\")\n",
        "    except HttpError as error:\n",
        "        print(f\"An HTTP error occurred while checking/downloading from Drive: {error}\")\n",
        "        print(\"Assuming no existing file in Drive for now.\")\n",
        "        file_id_in_drive = None # Ensure file_id_in_drive is None on error\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during Drive file check/download: {e}\")\n",
        "        print(\"Assuming no existing file in Drive for now.\")\n",
        "        file_id_in_drive = None # Ensure file_id_in_drive is None on error\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Determine the start date for fetching new data\n",
        "    if last_created_date_in_drive: # Use date from Drive if available\n",
        "        fetch_start_date = (last_created_date_in_drive + timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "    else: # Otherwise, use checkpoint or default\n",
        "        last_processed_date_str = read_last_processed_date()\n",
        "        fetch_start_date = (datetime.strptime(last_processed_date_str, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "    # If fetch_start_date is before DEFAULT_INITIAL_FETCH_DATE, clamp it\n",
        "    if datetime.strptime(fetch_start_date, '%Y-%m-%d') < datetime.strptime(DEFAULT_INITIAL_FETCH_DATE, '%Y-%m-%d'):\n",
        "        fetch_start_date = DEFAULT_INITIAL_FETCH_DATE\n",
        "        print(f\"Adjusted fetch start date to DEFAULT_INITIAL_FETCH_DATE: {fetch_start_date}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Determining data fetch strategy. Actual fetch start date: {fetch_start_date}\")\n",
        "\n",
        "\n",
        "    # --- Step 3: Fetch Data (Historical Chunks or Incremental) ---\n",
        "    all_fetched_data_frames = []\n",
        "\n",
        "\n",
        "    # This logic now primarily handles the initial full load or large gaps\n",
        "    # If the Google Sheet is empty in Drive OR the last processed date is significantly old (e.g., from a previous year)\n",
        "    # OR if the checkpoint date is very old, perform historical chunking.\n",
        "    current_year = datetime.now().year\n",
        "    start_year_for_chunking = datetime.strptime(fetch_start_date, '%Y-%m-%d').year\n",
        "\n",
        "\n",
        "    # Only perform chunking if we're starting from a historical year (not just a few days ago in current year)\n",
        "    # And if we haven't completed the historical load yet.\n",
        "    # The condition `not existing_df.empty` here is crucial: if existing_df is empty, it means Drive file was not found or empty\n",
        "    # so we need to do a full historical pull.\n",
        "    if not existing_df.empty and start_year_for_chunking < current_year: # Only chunk if existing_df is NOT empty and we are in a historical year\n",
        "        print(\"Performing historical data fetch in yearly chunks...\")\n",
        "        for year in range(start_year_for_chunking, current_year + 1):\n",
        "            year_start_date = f\"{year}-01-01\"\n",
        "            year_end_date = f\"{year}-12-31\"\n",
        "\n",
        "\n",
        "            if year == current_year:\n",
        "                year_end_date = datetime.now().strftime('%Y-%m-%d')\n",
        "                if datetime.strptime(year_start_date, '%Y-%m-%d') > datetime.strptime(year_end_date, '%Y-%m-%d'):\n",
        "                    continue\n",
        "\n",
        "\n",
        "            print(f\"\\n--- Fetching data for year: {year} ({year_start_date} to {year_end_date}) ---\")\n",
        "\n",
        "\n",
        "            # Only fetch if the year's start date is relevant to the overall fetch_start_date\n",
        "            if datetime.strptime(year_end_date, '%Y-%m-%d') >= datetime.strptime(fetch_start_date, '%Y-%m-%d'):\n",
        "                # Adjust year_start_date if it's earlier than fetch_start_date (for first relevant chunk)\n",
        "                actual_chunk_start_date = max(datetime.strptime(year_start_date, '%Y-%m-%d'), datetime.strptime(fetch_start_date, '%Y-%m-%d')).strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "                chunk_df = fetch_nyc_data_incremental(actual_chunk_start_date, year_end_date)\n",
        "                if not chunk_df.empty:\n",
        "                    all_fetched_data_frames.append(chunk_df)\n",
        "                else:\n",
        "                    print(f\"No data found for year {year} or fetch failed.\")\n",
        "            else:\n",
        "                print(f\"Skipping year {year} as it's older than the required fetch_start_date ({fetch_start_date}).\")\n",
        "\n",
        "\n",
        "        if not all_fetched_data_frames:\n",
        "            print(\"No new data fetched during historical chunking. Google Sheet will not be updated.\")\n",
        "            return\n",
        "\n",
        "\n",
        "        new_data_df = pd.concat(all_fetched_data_frames, ignore_index=True)\n",
        "        print(f\"Total records fetched during historical chunking: {len(new_data_df)}\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(f\"Performing daily incremental data fetch from: {fetch_start_date}...\")\n",
        "        new_data_df = fetch_nyc_data_incremental(fetch_start_date)\n",
        "        if new_data_df.empty:\n",
        "            print(\"No new data fetched from API for incremental update. Google Sheet will not be updated.\")\n",
        "            return\n",
        "\n",
        "\n",
        "    # --- Step 3: Combine existing data with new data ---\n",
        "    # Ensure column order is consistent before concat\n",
        "    if not existing_df.empty:\n",
        "        new_data_df = new_data_df.reindex(columns=existing_df.columns, fill_value=None)\n",
        "        combined_df = pd.concat([existing_df, new_data_df], ignore_index=True)\n",
        "    else:\n",
        "        combined_df = new_data_df\n",
        "\n",
        "\n",
        "    # Optional: Remove potential duplicates if 'unique_key' exists\n",
        "    if 'unique_key' in combined_df.columns:\n",
        "        initial_rows = len(combined_df)\n",
        "        combined_df.drop_duplicates(subset=['unique_key'], inplace=True, keep='last')\n",
        "        if len(combined_df) < initial_rows:\n",
        "            print(f\"Removed {initial_rows - len(combined_df)} duplicate rows based on 'unique_key'.\")\n",
        "\n",
        "\n",
        "    print(f\"Total rows in combined DataFrame: {len(combined_df)}\")\n",
        "\n",
        "\n",
        "    # --- Step 4: Prepare CSV content in memory for upload ---\n",
        "    csv_buffer = io.StringIO()\n",
        "    # Write to CSV in-memory, then encode to bytes\n",
        "    combined_df.to_csv(csv_buffer, index=False, encoding='utf-8')\n",
        "    csv_buffer.seek(0) # Rewind to the beginning of the buffer\n",
        "\n",
        "\n",
        "    # --- Step 5: Upload/Update Google Sheet file to Google Drive ---\n",
        "    print(\"Uploading/Updating Google Sheet file to Google Drive...\")\n",
        "\n",
        "\n",
        "    # Use file_id_in_drive to determine if update or create\n",
        "    if file_id_in_drive: # File was found and downloaded in Step 1\n",
        "        # Update existing file\n",
        "        file_metadata = {'name': GSHEET_FILE_NAME} # Name is sufficient for update\n",
        "\n",
        "\n",
        "        # Use MediaIoBaseUpload for in-memory content (CSV bytes)\n",
        "        media = MediaIoBaseUpload(\n",
        "            io.BytesIO(csv_buffer.getvalue().encode('utf-8')), # Pass the BytesIO object directly\n",
        "            mimetype='text/csv', # Source MIME type is CSV\n",
        "            resumable=True\n",
        "        )\n",
        "\n",
        "\n",
        "        try: # Added try-except for the update operation\n",
        "            updated_file = drive_service.files().update(\n",
        "                fileId=file_id_in_drive, # Use the ID of the found file\n",
        "                body=file_metadata, # Pass file_metadata as body\n",
        "                includeItemsFromAllDrives=True,\n",
        "                supportsAllDrives=True,\n",
        "                media_body=media,\n",
        "                # For converting CSV to native Google Sheet on update, use uploadType=resumable\n",
        "                # and ensure the body has the correct MIME type for conversion.\n",
        "                # If it's already a GSheet, just updating its content is fine.\n",
        "                # If it was a CSV and you want it to become a GSheet, you'd need to create a new GSheet.\n",
        "                fields='id, name'\n",
        "            ).execute()\n",
        "            print(f\"Updated existing Google Sheet on Google Drive (ID: {updated_file.get('id')}).\")\n",
        "            # Write checkpoint after successful upload\n",
        "            write_last_processed_date(datetime.now().strftime('%Y-%m-%d'), drive_service)\n",
        "        except HttpError as update_error:\n",
        "            if update_error.resp.status == 404:\n",
        "                print(f\"Warning: Google Sheet with ID {file_id_in_drive} not found during update. Attempting to create a new one instead.\")\n",
        "                # If 404 on update, proceed to create a new file\n",
        "                file_id_in_drive = None # Reset ID so it falls into the 'else' block for creation\n",
        "            else:\n",
        "                raise # Re-raise other HttpErrors\n",
        "\n",
        "\n",
        "    if not file_id_in_drive: # If file was not found initially, or 404 on update\n",
        "        # Upload new file (as a native Google Sheet)\n",
        "        file_metadata = {\n",
        "            'name': GSHEET_FILE_NAME,\n",
        "            'parents': [GOOGLE_DRIVE_FOLDER_ID],\n",
        "            'mimeType': GSHEET_MIMETYPE # Set MIME type for native Google Sheet\n",
        "        }\n",
        "\n",
        "\n",
        "        # Use MediaIoBaseUpload for in-memory content (CSV bytes)\n",
        "        media = MediaIoBaseUpload(\n",
        "            io.BytesIO(csv_buffer.getvalue().encode('utf-8')), # Pass the BytesIO object directly\n",
        "            mimetype='text/csv', # Source MIME type is CSV\n",
        "            resumable=True\n",
        "        )\n",
        "\n",
        "\n",
        "         # Create a new file, Google Drive will convert the CSV to a native Google Sheet\n",
        "        new_file = drive_service.files().create(\n",
        "            body=file_metadata,\n",
        "            media_body=media,\n",
        "            supportsAllDrives=True,\n",
        "            fields='id, name'\n",
        "        ).execute()\n",
        "        print(f\"Uploaded new Google Sheet to Google Drive (ID: {new_file.get('id')}).\")\n",
        "        # Write checkpoint after successful upload (for new file creation) then save to Github\n",
        "        write_last_processed_date(datetime.now().strftime('%Y-%m-%d'), drive_service)\n",
        "\n",
        "\n",
        "    print(\"--- Google Sheet Update and Google Drive Upload Process Completed ---\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    update_csv_and_upload_to_drive()"
      ]
    }
  ]
}